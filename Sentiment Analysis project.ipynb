{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Importing Libraries\n",
    "\n",
    "\n",
    "This is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the twitter api . \n",
    "The tweets have been annotated (0 = negative, 4 = positive) and they can be used to detect sentiment .\n",
    "\n",
    "Content\n",
    "It contains the following 6 fields:\n",
    "\n",
    "    1. target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "    2. ids: The id of the tweet ( 2087)\n",
    "    3. date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "    4. flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "    5. user: the user that tweeted (robotickilldozr)\n",
    "    6. text: the text of the tweet (Lyx is cool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "#methods and stopwords text processing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Creating a stopwords set\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filepath,cols):\n",
    "    '''\n",
    "    reads the CSV file to return\n",
    "    a dataframe with specified column names\n",
    "    '''\n",
    "    df=pd.read_csv(filepath,encoding='latin-1')\n",
    "    df.columns=cols\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting rid of unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_redundant_cols(df,cols):\n",
    "    '''\n",
    "    Delete unwanted columns(cols) from the dataframe.\n",
    "    '''\n",
    "    for col in cols:\n",
    "        del df[col]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Tasks\n",
    "    \n",
    "    1. Handling casing (all lower cases)\n",
    "    2. Noise Removal (Special characters, html tags, etc)\n",
    "    3. Tokenization\n",
    "    4. Stopword removal (The words which do not make sense are removed out)\n",
    "    5. Text Normalization (Stemming and Lemmatization)\n",
    "    \n",
    "            1. Stemming includes eliminating the affixes (prefixes, suffixes, infixes). Basically to reach the stem of the word or the root meaning of that word. Stemming sometimes loses the actual meaning of the word. Lemmatization here is better because it reduces the infected word properly by ensuring its morphological analysis and vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
